---
title: Metrics of Aging - Phenotypic Aging in BLSA  
author: Perry Kuo, Luigi Ferrucci, Jennifer Schrack, Michelle Shardell, Morgan Levine
date: Oct 1, 2018
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
fig_width: 5
fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", out.width = '70%')
```
The format is adopted from course led by [Dr. Stephanie Hicks](https://jhu-advdatasci.github.io/2018/)

# Motivation 

For the next three `lectures, we will be analyzing health
care data in the United States. We will be exploring 
the following questions: 

1. Is there a relationship between healthcare coverage and healthcare spending in the United States? 
2. Which US states spend the most and which spend the least on healthcare? How does the spending distribution change across geographic regions in the United States?
3. How do healthcare coverage and spending relate to life expectancy? 

## Healthcare data

We will be using the data from the [Henry J Kaiser Family Foundation (KFF)](https://www.kff.org). 

* [Health Insurance Coverage of the Total Population](https://www.kff.org/other/state-indicator/total-population/) - Includes years 2013-2016
* [Health Care Expenditures by State of Residence (in millions)](https://www.kff.org/other/state-indicator/health-care-expenditures-by-state-of-residence-in-millions/) - Includes years 1991-2014
* [Life Expectancy at Birth (in years)](https://www.kff.org/other/state-indicator/life-expectancy)

We have downloaded, re-named and saved these files in our course 
GitHub repository under the `data/KFF/` directory. 

Now, before we dig into the data analysis, we need to introduce 
a set of R packages that we will use to analyze the data. 

# Welcome to the "Tidyverse"

The [tidyverse](https://www.tidyverse.org) is _"an opinionated 
collection of R packages designed for data science. All packages 
share an underlying philosophy and common APIs."_ 

Another way of putting it is that it's a set of packages 
that are useful specifically for data manipulation, 
exploration and visualization with a common philosphy. 

## What is this common philosphy? 

The common philosphy is called _"tidy"_ data. It is 
a standard way of mapping the meaning of a dataset
to its structure.
  

## What is in the `tidyverse`? 


Next, we will give a brief description of the 
features in each of these packages. 

# Data Import

There are several base R functions that allow you 
read in data into R, which you may be familiar 
with such as `read.table()`, `read.csv()`, 
and `read.delim()`. Instead of using these, 
we will use the functions in the 
[readr](https://readr.tidyverse.org/articles/readr.html)
R package. The main reasons for this are 

1. Compared to equivalent base R functions, the 
functions in `readr` are around 10x faser. 
2. You can specify the column types (e.g 
character, integer, double, logical, date, 
time, etc)
3. All parsing problems are recordered in 
a data frame. 

## The `readr` R package

The main functions in `readr` are: 

If we want to see what the header of the file looks like, 
we can use the `read_lines()` function to peak at the 
first few lines. 


## Read in life expectancy data

Now because we are also going to want to 
use in `life-expectancy.csv`, let's 
read it in now. 

## Take a `glimpse()` at your data

One last thing in this section. 
One way to look at our data would be to use 
`head()` or `tail()`, as we just saw. 
Another one you might have heard of is the
`str()` function. One you might not have 
heard of is the `glimpse()` function. It's
used for a special type of object in R called 
a `tibble`. Let's read the help file to learn
more. 



# Data Tidying




Now, let's use the `tidyr` R package to transform
our data into a _tidy_ format. 

## The `tidyr` R package

[`tidyr`](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html)
is an R package that transforms data sets to a tidy format. 

This package is installed and loaded when you load 
the `tidyverse` using `library(tidyverse)`. However, 
you can also just load the library by itself. 


## Convert healthcare coverage data to a tidy format
This function gathers multiple columns and collapses them into new 
*key-value* pairs. This transform data from _wide_ format into 
a _long_  format. 

* The `key` is the name of the _new_ column that you are creating which 
contains the values of the column headings that you are gathering 
* The `value` is the name of the _new_ column that will contain the values
themselves
* The third argument defines the columns to gather

For example, here we create a column titled 
`year_type` and `coverage`. We also want to keep 
the `Location` column as it is because it also contains
observational level data.


Now we see each row contains one observation. 
Namely, a `Location`, a `year_type` and `coverage`. 
It would be nice to separate out the information 
in the `year_type` column into two columns. We will 
explore how to do that in the Data Wrangling section
below. For now let's learn more about the `tidyr` 
package. 

### Convert back to a wide format

In contrast to *gathering* multiple columns into key-value pairs, we can 
*spread* a key-value pair across multiple columns.  

The function `spread()` does just that. It transforms data from a _long_
format into a _wide_ format. 

* The `key` is the name of the column in your data set that 
contains the values of the column headings that you are spreading across 
multiple columns
* The `value` is the name of the column that contains the values for the 
multiple columns

## Convert healthcare spending data to a tidy format

Let's do the same for the `spending` data. In this 
case I will use `year` and `spending` for
the `key` and `value`. We also want to keep `Location`
like before. 


# Data Wrangling

In the real world, analyzing data rarely involves 
data that can be easily imported and ready for 
analysis. According to Wikipedia:

> Data munging or data wrangling is loosely the process 
of manually converting or mapping data from one "raw" 
form into another format that allows for more convenient 
consumption of the data with the help of semi-automated 
tools.

As you will see in class, one of the most 
time-consuming aspects of the data analysis 
process is "data wrangling". This is also 
is a trendy term for 
_cleaning up a messy data set_. 

R provides incredibly powerful and flexible language 
for data wrangling. However, the syntax is somewhat 
hard to get used to. We will therefore introducing 
a package that makes the syntax much more like 
the English language. This package is `dplyr`. 

## The `dplyr` R package

[`dplyr`](http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) 
is a powerful R-package to transform and summarize 
tabular data with rows and columns. 

The package contains a set of functions 
(or "verbs") to perform common data manipulation
operations such as filtering for rows, selecting 
specific columns, re-ordering rows, adding new 
columns and summarizing data. 

In addition, `dplyr` contains a useful function to
perform another common task which is the is the 
"split-apply-combine" concept.  We will discuss 
that in a little bit. 

### How does it compare to using base functions R?

If you are familiar with R, you are probably familiar 
with base R functions such as `split()`, `subset()`, 
`apply()`, `sapply()`, `lapply()`, `tapply()` and 
`aggregate()`. Compared to base functions in R, the 
functions in `dplyr` are easier to work with, are 
more consistent in the syntax and are targeted for 
data analysis around data frames instead of just vectors. 

The important `dplyr` verbs to remember are: 

`dplyr` verbs | Description
--- | ---
`select()` | select columns 
`filter()` | filter rows
`arrange()` | re-order or arrange rows
`mutate()` | create new columns
`summarize()` | summarize values
`group_by()` | allows for group operations in the "split-apply-combine" concept



### Pipe operator: %>%

Before we go any futher, let's introduce the 
pipe operator: `%>%`. In our `stocks` example,
we briefly saw this symbol. It is called the
pipe operator. `dplyr` imports
this operator from another package 
(`magrittr`)
[see help file here](http://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html). 
This operator allows you to pipe the output 
from one function to the input of another
function. Instead of nesting functions 
(reading from the inside to the 
outside), the idea of of piping is to 
read the functions from left to right. 

Now in `stocks` example, we pipe the `stocks`
data frame to the function that will 
gather multiple columns into key-value pairs. 

![](https://github.com/datasciencelabs/2016/raw/master/lectures/wrangling/pics/stocks-tidy.png)


### `dplyr` verbs in action

First, let's separate the `year_type` column 
in the `coverage` dataset to two columns:
`year` and health coverage `type`. 

To do this, we will use the `separate()` 
function in the `tidyr` package. 

**Note**: 

* `separate()` = separate one column into multiple columns
* `unite()` = unite multiple columns into one

We see that we now have two columns, except 
the `year` column was converted to a character. 
If we look at the help file `?separate`, we see
we can use the `convert=TRUE` argument to 
convert the character to an integer. 

Next, we see that the `tot_coverage` column is 
also a character. Gah! 

Let's fix that. We can use the `mutate_at()` 
function to do this. We are asking R to take
`tot_coverage` column and convert it to an
integer and then replace the old column with 
the new converted column 

The `coverage` data looks good now. We see 
that there are different `year`s and different 
`types` of healthcare coverage. 

#### Your turn

What is the range of years and types of healthcare
in the `coverage` dataset? 

Next, we will look at the life expectancy
(`life`) data. 

The second column 
has a long name with space in it. Let's rename
it to something shorter with the `rename()` 
function. 


Let's use the `separate()` function with `convert=TRUE` 
to separate the `year` column into columns. Then, we 
introduce another `dplyr` action verb: `select()`. 

The two most basic functions are `select()` and 
`filter()` which selects columns and filters 
rows, respectively. 

#### Selecting columns using `select()`

In the `separate()` function, we create two
new columns called `year` and `name`. Then, 
we ask to return all the columns, except 
`name`. To select all the columns *except* a 
specific column, use the "-" (subtraction) operator 
(also known as negative indexing). 

Some additional options to select columns based 
on a specific criteria include

1. `ends_with()` = Select columns that end with 
a character string
2. `contains()` = Select columns that contain 
a character string
3. `matches()` = Select columns that match a 
regular expression
4. `one_of()` = Select columns names that are 
from a group of names


#### Selecting rows using `filter()`

Let's say we want to know how many peopled 
had health insurance coverage in Maryland? 

First, we can filter the rows for years in 2007. 


#### Your turn

Has the number of uninsured has
increased or decreased in Maryland 
between 2013 and 2016? 


What happened between 2013 and 2014? 
[Hint](https://en.wikipedia.org/wiki/Patient_Protection_and_Affordable_Care_Act)

#### Arrange or re-order rows using `arrange()`

Now, let's say we want to see which states has the To arrange (or re-order) rows by a particular 
column such as the population, list the name of 
the column you want to arrange the rows by



#### Your turn 

In 2016, what were the top three states with 
the largest `Employer` type of healthcare 
coverage? 

**Hint**: use the `desc()` function inside of
`arrange()` to order rows in a descending order. 


# Mini-Case Study 1 

For the last 20 mins of class, we will break 
into groups and work on a mini-case study. 
You can find the case study on the 
[course website](https://jhu-advdatasci.github.io/2018/). 
